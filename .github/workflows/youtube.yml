name: YouTube Multi-Channel Daily Tracker

on:
  schedule:
    - cron: '00 13 * * *'   # 每天 UTC 13:00 = 北京时间 21:00
  workflow_dispatch:

jobs:
  fetch:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: pip install google-api-python-client

      - name: Fetch latest videos for multiple channels
        env:
          YT_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        run: |
          python <<'PY'
          import os, json, datetime
          from zoneinfo import ZoneInfo
          from googleapiclient.discovery import build
          from googleapiclient.errors import HttpError

          API_KEY = os.environ["YT_KEY"]
          yt = build("youtube", "v3", developerKey=API_KEY)

          # === 追踪的频道列表 ===
          HANDLES = [
            "@biubiubuff",
            "@LittleZGallery",
            "@HiLanbuff",
            "@AIFunnny-777",
            "@Starway-v5z",
            "@Realcomicheros",
            "@helloAI777",
            "@DogsFantasy-DF",
            "@BabyPrincess-i3i",
            "@CharlieMagicShow"
          ]

          PER_CHANNEL = 50

          def get_channel_id_from_handle(handle: str) -> str:
            try:
              resp = yt.channels().list(part="id", forHandle=handle).execute()
              items = resp.get("items", [])
              if items:
                return items[0]["id"]
            except HttpError:
              pass
            s = yt.search().list(part="snippet", q=handle, type="channel", maxResults=1).execute()
            items = s.get("items", [])
            if items:
              return items[0]["snippet"]["channelId"]
            raise RuntimeError(f"无法解析频道ID：{handle}")

          def fetch_latest_videos(channel_id: str, limit: int = 50):
            videos = []
            page_token = None
            fetched = 0
            while fetched < limit:
              page_size = min(50, limit - fetched)
              search = yt.search().list(
                  part="snippet",
                  channelId=channel_id,
                  maxResults=page_size,
                  order="date",
                  type="video",
                  pageToken=page_token
              ).execute()
              items = search.get("items", [])
              if not items:
                break
              ids = [it["id"]["videoId"] for it in items if it["id"].get("videoId")]
              if not ids:
                break

              stats_resp = yt.videos().list(part="statistics", id=",".join(ids)).execute()
              stats_map = {v["id"]: v.get("statistics", {}) for v in stats_resp.get("items", [])}

              for it in items:
                vid = it["id"].get("videoId")
                if not vid: continue
                title = it["snippet"]["title"]
                desc = it["snippet"].get("description", "")
                published_utc = it["snippet"]["publishedAt"]
                dt_utc = datetime.datetime.strptime(published_utc, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=ZoneInfo("UTC"))
                dt_bj = dt_utc.astimezone(ZoneInfo("Asia/Shanghai"))
                views = stats_map.get(vid, {}).get("viewCount", "0")

                videos.append({
                  "video_id": vid,
                  "title": title,
                  "url": f"https://youtu.be/{vid}",
                  "description": desc,
                  "published_beijing": dt_bj.strftime("%Y-%m-%d %H:%M:%S"),
                  "views": views
                })

              fetched += len(items)
              page_token = search.get("nextPageToken")
              if not page_token:
                break

            videos.sort(key=lambda x: x["published_beijing"], reverse=True)
            return videos

          result = {
            "generated_at_beijing": datetime.datetime.now(ZoneInfo("Asia/Shanghai")).strftime("%Y-%m-%d %H:%M:%S"),
            "channels": []
          }

          for handle in HANDLES:
            try:
              cid = get_channel_id_from_handle(handle)
              vids = fetch_latest_videos(cid, PER_CHANNEL)
              result["channels"].append({
                "handle": handle,
                "channel_id": cid,
                "count": len(vids),
                "videos": vids
              })
            except Exception as e:
              result["channels"].append({
                "handle": handle,
                "channel_id": "~",
                "error": str(e),
                "videos": []
              })

          with open("channels.json", "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
          PY

      - name: Commit & Push JSON
        run: |
          git config user.name "auto"
          git config user.email "auto@users.noreply.github.com"
          git add channels.json
          git commit -m "update $(date -u)" || echo "no changes"
          git push

      - name: Generate Beijing Time for Notification
        id: date
        run: |
          echo "beijing=$(TZ='Asia/Shanghai' date '+%Y年%m月%d日（周%u）北京时间 %H:%M')" >> $GITHUB_OUTPUT

      - name: Send email notification
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.qq.com
          server_port: 465
          username: ${{ secrets.MAIL_USER }}
          password: ${{ secrets.MAIL_PASS }}
          subject: "📊 印度AI短视频增长情报日报｜${{ steps.date.outputs.beijing }}"
          to: 3200591741@qq.com
          from: "频道日报提醒机器人 <${{ secrets.MAIL_USER }}>"
          secure: true
          content_type: text/html
          body: |
            <p>📅 推送时间｜${{ steps.date.outputs.beijing }}</p>
            <p>✅ 数据已更新，请打开 ChatGPT 查看完整分析报告。</p>
            <p>🔗 JSON 数据源链接：</p>
            <p><a href="https://raw.githubusercontent.com/HZUPersianCat/youtube-tracker/main/channels.json">channels.json</a></p>
            <p>📌 GPT 将基于该数据自动输出结构化日报，包括新视频、爆款标记、趋势表格与总结，请留意 ChatGPT 推送。</p>
