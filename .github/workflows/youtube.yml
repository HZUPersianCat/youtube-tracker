name: YouTube Multi-Channel Data Refresh (3x Daily)

on:
  schedule:
    - cron: '0 0 * * *'     # 北京 08:00
    - cron: '0 6 * * *'     # 北京 14:00
    - cron: '30 12 * * *'   # 北京 20:30
  workflow_dispatch: {}

jobs:
  fetch:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: pip install google-api-python-client

      - name: Fetch latest videos for multiple channels
        env:
          YT_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        run: |
          python <<'PY'
          import os, json, datetime
          from zoneinfo import ZoneInfo
          from googleapiclient.discovery import build
          from googleapiclient.errors import HttpError

          API_KEY = os.environ["YT_KEY"]
          yt = build("youtube", "v3", developerKey=API_KEY)

          # === 追踪的频道列表 ===
          HANDLES = [
            "@biubiubuff",
            "@LittleZGallery",
            "@HiLanbuff",
            "@AIFunnny-777",
            "@Starway-v5z",
            "@Realcomicheros",
            "@helloAI777",
            "@DogsFantasy-DF",
            "@BabyPrincess-i3i",
            "@CharlieMagicShow"
          ]

          PER_CHANNEL = 50  # 如需更快，可改为 30/20

          def get_channel_id_from_handle(handle: str) -> str:
            try:
              resp = yt.channels().list(part="id", forHandle=handle).execute()
              items = resp.get("items", [])
              if items:
                return items[0]["id"]
            except HttpError:
              pass
            s = yt.search().list(part="snippet", q=handle, type="channel", maxResults=1).execute()
            items = s.get("items", [])
            if items:
              return items[0]["snippet"]["channelId"]
            raise RuntimeError(f"无法解析频道ID：{handle}")

          def fetch_latest_videos(channel_id: str, limit: int = 50):
            videos = []
            page_token = None
            fetched = 0
            while fetched < limit:
              page_size = min(50, limit - fetched)
              search = yt.search().list(
                  part="snippet",
                  channelId=channel_id,
                  maxResults=page_size,
                  order="date",
                  type="video",
                  pageToken=page_token
              ).execute()
              items = search.get("items", [])
              if not items:
                break
              ids = [it["id"]["videoId"] for it in items if it["id"].get("videoId")]
              if not ids:
                break

              stats_resp = yt.videos().list(part="statistics", id=",".join(ids)).execute()
              stats_map = {v["id"]: v.get("statistics", {}) for v in stats_resp.get("items", [])}

              for it in items:
                vid = it["id"].get("videoId")
                if not vid: 
                  continue
                title = it["snippet"]["title"]
                desc = it["snippet"].get("description", "")
                published_utc = it["snippet"]["publishedAt"]
                dt_utc = datetime.datetime.strptime(published_utc, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=ZoneInfo("UTC"))
                dt_bj = dt_utc.astimezone(ZoneInfo("Asia/Shanghai"))
                views = int(stats_map.get(vid, {}).get("viewCount", "0") or 0)

                # 精简字段：去掉大型 description（如保留请改为 desc[:200]）
                videos.append({
                  "video_id": vid,
                  "title": title,
                  "url": f"https://youtu.be/{vid}",
                  "published_beijing": dt_bj.strftime("%Y-%m-%d %H:%M:%S"),
                  "views": views
                })

              fetched += len(items)
              page_token = search.get("nextPageToken")
              if not page_token:
                break

            # 按发布时间倒序
            videos.sort(key=lambda x: x["published_beijing"], reverse=True)
            return videos

          result = {
            "generated_at_beijing": datetime.datetime.now(ZoneInfo("Asia/Shanghai")).strftime("%Y-%m-%d %H:%M:%S"),
            "channels": []
          }

          for handle in HANDLES:
            try:
              cid = get_channel_id_from_handle(handle)
              vids = fetch_latest_videos(cid, PER_CHANNEL)
              result["channels"].append({
                "handle": handle,
                "channel_id": cid,
                "count": len(vids),
                "videos": vids
              })
            except Exception as e:
              result["channels"].append({
                "handle": handle,
                "channel_id": "~",
                "error": str(e),
                "videos": []
              })

          # 紧凑 JSON，体积更小（更利于 o3 加载）
          with open("channels.json", "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, separators=(",", ":"))
          PY

      - name: Commit & Push JSON
        run: |
          git config user.name "auto"
          git config user.email "auto@users.noreply.github.com"
          git add channels.json
          git commit -m "update $(date -u)" || echo "no changes"
          git push
